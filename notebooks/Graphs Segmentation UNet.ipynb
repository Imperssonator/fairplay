{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rCSP-dbMw88x"
   },
   "source": [
    "# Image segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "YQX7R4bhZy5h"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 2.16.2\n",
      "❌ No GPU found. TensorFlow is using the CPU.\n",
      "date and time = 10-10-2025-14:59:33\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "gpu_devices = tf.config.list_physical_devices('GPU')\n",
    "\n",
    "print(f\"TensorFlow Version: {tf.__version__}\")\n",
    "\n",
    "if gpu_devices:\n",
    "    print(f\"✅ Found {len(gpu_devices)} GPU(s):\")\n",
    "    for device in gpu_devices:\n",
    "        print(f\"  - {device}\")\n",
    "else:\n",
    "    print(\"❌ No GPU found. TensorFlow is using the CPU.\")\n",
    "\n",
    "# dd/mm/YY H:M:S\n",
    "dt_string = datetime.now().strftime(\"%d-%m-%Y-%H:%M:%S\")\n",
    "print(\"date and time =\", dt_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oWe0_rQM4JbC"
   },
   "source": [
    "## Create a Dataset from the folder of train/val images\n",
    "\n",
    "And make a directory to save outputs on Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pWFcgafGmdZf"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "code_root = Path('/content/drive/MyDrive/10 - code/')\n",
    "ds_path = code_root.joinpath('Semantic-Segmentation-Suite/data/210221-colab-test-1')\n",
    "print(\"Dataset folder:\")\n",
    "print(ds_path.as_posix())\n",
    "\n",
    "outputs_path = code_root.joinpath('Colab Data Xfer/{}'.format(datetime.now().strftime(\"%Y-%m-%d\")))\n",
    "displays_path = outputs_path.joinpath('displays')\n",
    "checkpoints_path = outputs_path.joinpath('checkpoints')\n",
    "outputs_path.mkdir(parents=True, exist_ok=True)\n",
    "displays_path.mkdir(parents=True, exist_ok=True)\n",
    "checkpoints_path.mkdir(parents=True, exist_ok=True)\n",
    "print(\"Outputs folder:\")\n",
    "print(outputs_path.as_posix())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oJEQZD9ySh1H"
   },
   "outputs": [],
   "source": [
    "list_ds = tf.data.Dataset.list_files(ds_path.joinpath('train/*.png').as_posix())\n",
    "test_list_ds = tf.data.Dataset.list_files(ds_path.joinpath('val/*.png').as_posix())\n",
    "list_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tjYXV1_5C3dB"
   },
   "outputs": [],
   "source": [
    "df_classes = pd.read_csv('/content/drive/MyDrive/10 - code/Semantic-Segmentation-Suite/data/210221-colab-test-1/class_dict.csv')\n",
    "class_dict = {r['name']:np.array((r.r, r.g, r.b)) for ii,r in df_classes.iterrows()}\n",
    "class_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IDcj2ZIiYEp3"
   },
   "outputs": [],
   "source": [
    "for f in list_ds.take(1):\n",
    "  print(f.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UGWbvg-38Tgy"
   },
   "outputs": [],
   "source": [
    "INPUT_SIZE = (512, 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6MaWt-slEPDW"
   },
   "outputs": [],
   "source": [
    "def mask_rgb_to_int(A):\n",
    "  \"\"\" Given image tensor A (h, w, 3) and class_dict {class: [r, g, b]},\n",
    "      build mask array M (h, w, 1) where values are integers corresponding\n",
    "      to each class\n",
    "      \"\"\"\n",
    "  class_dict = {\n",
    "    'background': np.array([255, 255, 212]),\n",
    "    # 'error_bars': np.array([54, 55, 55]),\n",
    "    'markers': np.array([  3,  67, 223]),\n",
    "    'x_tick_labels': np.array([229,   0,   0]),\n",
    "    'x_ticks': np.array([132,   0,   0]),\n",
    "    'y_tick_labels': np.array([191, 119, 246]),\n",
    "    'y_ticks': np.array([154,  14, 234])\n",
    "  }\n",
    "\n",
    "  A_png = (A.numpy()*255).astype(np.uint8)\n",
    "  M = np.zeros((A.shape[0], A.shape[1], 1), dtype=np.uint8)\n",
    "\n",
    "  for ii, (key, rgb) in enumerate(class_dict.items()):\n",
    "    M_i = np.sum(np.abs(A_png-rgb), axis=2)<1\n",
    "    M[M_i] = ii\n",
    "\n",
    "  return M\n",
    "\n",
    "def tf_mask_rgb_to_int(image):\n",
    "  im_shape = (image.shape[0], image.shape[1], 1)\n",
    "  [image,] = tf.py_function(mask_rgb_to_int, [image], [tf.uint8])\n",
    "  image.set_shape(im_shape)\n",
    "  return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mvAfJSBYmzXg"
   },
   "outputs": [],
   "source": [
    "def parse_image(filepath):\n",
    "  filename = tf.strings.split(filepath, os.sep)[-1]\n",
    "  label = tf.strings.split(filename, '.')[-2]\n",
    "\n",
    "  image_raw = tf.io.read_file(filepath)\n",
    "  image_png = tf.image.decode_png(image_raw, channels=3)\n",
    "  image_float = tf.image.convert_image_dtype(image_png, tf.float32)\n",
    "  image_float = tf.image.resize(image_float, INPUT_SIZE, method='nearest')\n",
    "\n",
    "  mask_float = get_segmentation_mask(filepath)\n",
    "  mask_int = tf_mask_rgb_to_int(mask_float)\n",
    "  return {'image_float':image_float, 'mask_float':mask_float, 'mask_int':mask_int, 'label':label}\n",
    "\n",
    "def get_segmentation_mask(image_filepath):\n",
    "  mask_filepath = tf.strings.regex_replace(image_filepath, 'train', 'train_labels')\n",
    "  mask_filepath = tf.strings.regex_replace(mask_filepath, 'val', 'val_labels')\n",
    "  mask_filename = tf.strings.split(mask_filepath, os.sep)[-1]\n",
    "  # label = tf.strings.split(mask_filename, '.')[-2]\n",
    "\n",
    "  mask_raw = tf.io.read_file(mask_filepath)\n",
    "  mask_png = tf.image.decode_png(mask_raw, channels=3)\n",
    "  mask_float = tf.image.convert_image_dtype(mask_png, tf.float32)\n",
    "  mask_float = tf.image.resize(mask_float, INPUT_SIZE, method='nearest')\n",
    "  return mask_float\n",
    "\n",
    "def show(image, label):\n",
    "  plt.figure()\n",
    "  plt.imshow(image)\n",
    "  plt.title(label.numpy().decode('utf-8'))\n",
    "  plt.axis('off')\n",
    "\n",
    "def display_init(display_list, fig_size=5):\n",
    "  plt.figure(figsize=(len(display_list) * fig_size, fig_size), tight_layout=True)\n",
    "\n",
    "  title = ['Input Image', 'True Mask RGB', 'True Mask int']\n",
    "\n",
    "  for i in range(len(display_list)):\n",
    "    plt.subplot(1, len(display_list), i+1)\n",
    "    plt.title(title[i])\n",
    "    plt.imshow(tf.keras.preprocessing.image.array_to_img(display_list[i]))\n",
    "    plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "90hgTEQgnkHh"
   },
   "outputs": [],
   "source": [
    "for f in list_ds.take(1):\n",
    "  dp = parse_image(f.numpy())\n",
    "  display_init([dp['image_float'], dp['mask_float'], dp['mask_int']])\n",
    "plt.savefig(displays_path.joinpath('mask_test_{}.png'.format(datetime.now().strftime(\"%Y-%m-%d-%H:%M:%S\"))).as_posix(), dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aBMdUCAOe256"
   },
   "outputs": [],
   "source": [
    "np.unique(dp['mask_int'].numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nXwMdu5QY7rT"
   },
   "source": [
    "Debugging stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S99lDOlEL1tk"
   },
   "outputs": [],
   "source": [
    "# pd.DataFrame(dp['mask_int'].numpy()[:,:,0]).to_csv('array.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OjbwU-POLCkV"
   },
   "outputs": [],
   "source": [
    "# print((mask_int_test.numpy()==5).sum())\n",
    "# print(np.all(mask_rgb_test.numpy()==class_dict['y_tick_labels'],axis=2).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rJcVdj_U4vzf"
   },
   "source": [
    "The following code performs a simple augmentation of flipping an image. In addition,  image is normalized to [0,1]. Finally, as mentioned above the pixels in the segmentation mask are labeled either {1, 2, 3}. For the sake of convenience, let's subtract 1 from the segmentation mask, resulting in labels that are : {0, 1, 2}."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NR64I3qQRnTu"
   },
   "outputs": [],
   "source": [
    "train_img_ds = list_ds.map(parse_image)\n",
    "test_img_ds = test_list_ds.map(parse_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2NPlCnBXQwb1"
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def load_image_train(datapoint):\n",
    "  input_image = datapoint['image_float']\n",
    "  input_mask = datapoint['mask_int']\n",
    "\n",
    "  if tf.random.uniform(()) > 0.5:\n",
    "    input_image = tf.image.flip_left_right(input_image)\n",
    "    input_mask = tf.image.flip_left_right(input_mask)\n",
    "\n",
    "  return input_image, input_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zf0S67hJRp3D"
   },
   "outputs": [],
   "source": [
    "def load_image_test(datapoint):\n",
    "  input_image = datapoint['image_float']\n",
    "  input_mask = datapoint['mask_int']\n",
    "\n",
    "  return input_image, input_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "65-qHTjX5VZh"
   },
   "source": [
    "The dataset already contains the required splits of test and train and so let's continue to use the same split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yHwj2-8SaQli"
   },
   "outputs": [],
   "source": [
    "TRAIN_LENGTH = 1000\n",
    "BATCH_SIZE = 16\n",
    "BUFFER_SIZE = 1000\n",
    "STEPS_PER_EPOCH = TRAIN_LENGTH // BATCH_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "39fYScNz9lmo"
   },
   "outputs": [],
   "source": [
    "train = train_img_ds.map(load_image_train, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "test = test_img_ds.map(load_image_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DeFwFDN6EVoI"
   },
   "outputs": [],
   "source": [
    "train_dataset = train.cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE).repeat()\n",
    "train_dataset = train_dataset.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "test_dataset = test.batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xa3gMAE_9qNa"
   },
   "source": [
    "Let's take a look at an image example and its correponding mask from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3N2RPAAW9q4W"
   },
   "outputs": [],
   "source": [
    "def display(display_list, fig_size=5, show=True):\n",
    "  fig = plt.figure(figsize=(len(display_list) * fig_size, fig_size))\n",
    "\n",
    "  title = ['Input Image', 'True Mask', 'Predicted Mask']\n",
    "\n",
    "  for i in range(len(display_list)):\n",
    "    plt.subplot(1, len(display_list), i+1)\n",
    "    plt.title(title[i])\n",
    "    plt.imshow(tf.keras.preprocessing.image.array_to_img(display_list[i]))\n",
    "    plt.axis('off')\n",
    "\n",
    "  plt.savefig(\n",
    "      displays_path.joinpath('display_{}.png'.format(datetime.now().strftime(\"%Y-%m-%d-%H:%M:%S\"))).as_posix(),\n",
    "      dpi=300\n",
    "      )\n",
    "\n",
    "  if not show:\n",
    "    plt.close();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WkdZwEcnYulu"
   },
   "outputs": [],
   "source": [
    "for image, mask in train.take(2):\n",
    "  sample_image, sample_mask = image, mask\n",
    "  display([sample_image, sample_mask]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Hfgobq8bXPmY"
   },
   "outputs": [],
   "source": [
    "for image, mask in test.take(2):\n",
    "  sample_image, sample_mask = image, mask\n",
    "  display([sample_image, sample_mask]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FAOe93FRMk3w"
   },
   "source": [
    "## Define the model\n",
    "The model being used here is a modified U-Net. A U-Net consists of an encoder (downsampler) and decoder (upsampler). In-order to learn robust features, and reduce the number of trainable parameters, a pretrained model can be used as the encoder. Thus, the encoder for this task will be a pretrained MobileNetV2 model, whose intermediate outputs will be used, and the decoder will be the upsample block already implemented in TensorFlow Examples in the [Pix2pix tutorial](https://github.com/tensorflow/examples/blob/master/tensorflow_examples/models/pix2pix/pix2pix.py).\n",
    "\n",
    "The reason to output three channels is because there are three possible labels for each pixel. Think of this as multi-classification where each pixel is being classified into three classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c6iB4iMvMkX9"
   },
   "outputs": [],
   "source": [
    "OUTPUT_CHANNELS = 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W4mQle3lthit"
   },
   "source": [
    "As mentioned, the encoder will be a pretrained MobileNetV2 model which is prepared and ready to use in [tf.keras.applications](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/applications). The encoder consists of specific outputs from intermediate layers in the model. Note that the encoder will not be trained during the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yqUYYu08_UNK"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "\n",
    "\n",
    "def get_model(img_size, num_classes):\n",
    "    inputs = tf.keras.Input(shape=img_size + (3,))\n",
    "\n",
    "    ### [First half of the network: downsampling inputs] ###\n",
    "\n",
    "    # Entry block\n",
    "    x = layers.Conv2D(32, 3, strides=2, padding=\"same\")(inputs)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation(\"relu\")(x)\n",
    "\n",
    "    previous_block_activation = x  # Set aside residual\n",
    "\n",
    "    # Blocks 1, 2, 3 are identical apart from the feature depth.\n",
    "    for filters in [64, 128, 256]:\n",
    "        x = layers.Activation(\"relu\")(x)\n",
    "        x = layers.SeparableConv2D(filters, 3, padding=\"same\")(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "\n",
    "        x = layers.Activation(\"relu\")(x)\n",
    "        x = layers.SeparableConv2D(filters, 3, padding=\"same\")(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "\n",
    "        x = layers.MaxPooling2D(3, strides=2, padding=\"same\")(x)\n",
    "\n",
    "        # Project residual\n",
    "        residual = layers.Conv2D(filters, 1, strides=2, padding=\"same\")(\n",
    "            previous_block_activation\n",
    "        )\n",
    "        x = layers.add([x, residual])  # Add back residual\n",
    "        previous_block_activation = x  # Set aside next residual\n",
    "\n",
    "    ### [Second half of the network: upsampling inputs] ###\n",
    "\n",
    "    for filters in [256, 128, 64, 32]:\n",
    "        x = layers.Activation(\"relu\")(x)\n",
    "        x = layers.Conv2DTranspose(filters, 3, padding=\"same\")(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "\n",
    "        x = layers.Activation(\"relu\")(x)\n",
    "        x = layers.Conv2DTranspose(filters, 3, padding=\"same\")(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "\n",
    "        x = layers.UpSampling2D(2)(x)\n",
    "\n",
    "        # Project residual\n",
    "        residual = layers.UpSampling2D(2)(previous_block_activation)\n",
    "        residual = layers.Conv2D(filters, 1, padding=\"same\")(residual)\n",
    "        x = layers.add([x, residual])  # Add back residual\n",
    "        previous_block_activation = x  # Set aside next residual\n",
    "\n",
    "    # Add a per-pixel classification layer\n",
    "    outputs = layers.Conv2D(num_classes, 3, activation=\"softmax\", padding=\"same\")(x)\n",
    "\n",
    "    # Define the model\n",
    "    model = tf.keras.Model(inputs, outputs)\n",
    "    return model\n",
    "\n",
    "\n",
    "# Free up RAM in case the model definition cells were run multiple times\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "# Build model\n",
    "model = get_model(INPUT_SIZE, OUTPUT_CHANNELS)\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j0DGH_4T0VYn"
   },
   "source": [
    "## Train the model\n",
    "Now, all that is left to do is to compile and train the model. The loss being used here is `losses.SparseCategoricalCrossentropy(from_logits=True)`. The reason to use this loss function is because the network is trying to assign each pixel a label, just like multi-class prediction. In the true segmentation mask, each pixel has either a {0,1,2}. The network here is outputting three channels. Essentially, each channel is trying to learn to predict a class, and `losses.SparseCategoricalCrossentropy(from_logits=True)` is the recommended loss for\n",
    "such a scenario. Using the output of the network, the label assigned to the pixel is the channel with the highest value. This is what the create_mask function is doing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6he36HK5uKAc"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xVMzbIZLcyEF"
   },
   "source": [
    "Have a quick look at the resulting model architecture:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sw82qF1Gcovr"
   },
   "outputs": [],
   "source": [
    "# tf.keras.utils.plot_model(model, show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tc3MiEO2twLS"
   },
   "source": [
    "Let's try out the model to see what it predicts before training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UwvIKLZPtxV_"
   },
   "outputs": [],
   "source": [
    "def create_mask(pred_mask):\n",
    "  pred_mask = tf.argmax(pred_mask, axis=-1)\n",
    "  pred_mask = pred_mask[..., tf.newaxis]\n",
    "  return pred_mask[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YLNsrynNtx4d"
   },
   "outputs": [],
   "source": [
    "def show_predictions(model, dataset=None, num=1, show=True):\n",
    "  if dataset:\n",
    "    for image, mask in dataset.take(num):\n",
    "      pred_mask = model.predict(image)\n",
    "      display([image[0], mask[0], create_mask(pred_mask)], show=show)\n",
    "  else:\n",
    "    display([sample_image, sample_mask,\n",
    "             create_mask(model.predict(sample_image[tf.newaxis, ...]))], show=show)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X_1CC0T4dho3"
   },
   "outputs": [],
   "source": [
    "show_predictions(model, dataset=test_dataset, num=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "22AyVYWQdkgk"
   },
   "source": [
    "Let's observe how the model improves while it is training. To accomplish this task, a callback function is defined below. Let's also save model weights while training to enable re-starting training..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wHrHsqijdmL6"
   },
   "outputs": [],
   "source": [
    "class DisplayCallback(tf.keras.callbacks.Callback):\n",
    "  def on_epoch_end(self, epoch, logs=None):\n",
    "    clear_output(wait=True)\n",
    "    show_predictions(model, dataset=test_dataset, num=3, show=False)\n",
    "    print ('\\nSample Prediction after epoch {}\\n'.format(epoch+1))\n",
    "\n",
    "\n",
    "checkpoint_path = checkpoints_path.joinpath(\"cp.ckpt\").as_posix()\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "print(\"Checkpoints path:\")\n",
    "print(checkpoint_path)\n",
    "\n",
    "# Create a callback that saves the model's weights\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
    "                                                 save_weights_only=True,\n",
    "                                                 verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g2D1nmUhUTgG"
   },
   "source": [
    "Ability to Load from Checkpoint here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4fDya2nkUTLI"
   },
   "outputs": [],
   "source": [
    "# Loads the weights\n",
    "old_checkpoint_path = outputs_path.joinpath(\"2023-12-18/checkpoints/cp.ckpt\"),\n",
    "model.load_weights(checkpoint_path)\n",
    "print(\"Weights loaded from checkpoint\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "StKDH_B9t4SD"
   },
   "outputs": [],
   "source": [
    "EPOCHS = 100\n",
    "VAL_SUBSPLITS = 2\n",
    "VALIDATION_STEPS = len(test_img_ds)//BATCH_SIZE//VAL_SUBSPLITS\n",
    "\n",
    "model_history = model.fit(train_dataset, epochs=EPOCHS,\n",
    "                          steps_per_epoch=STEPS_PER_EPOCH,\n",
    "                          validation_steps=VALIDATION_STEPS,\n",
    "                          validation_data=test_dataset,\n",
    "                          callbacks=[DisplayCallback(), cp_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P_mu0SAbt40Q"
   },
   "outputs": [],
   "source": [
    "loss = model_history.history['loss']\n",
    "val_loss = model_history.history['val_loss']\n",
    "\n",
    "epochs = range(EPOCHS)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, 'r', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'bo', label='Validation loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss Value')\n",
    "plt.ylim([0, 1])\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "unP3cnxo_N72"
   },
   "source": [
    "## Make predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7BVXldSo-0mW"
   },
   "source": [
    "Let's make some predictions. In the interest of saving time, the number of epochs was kept small, but you may set this higher to achieve more accurate results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aRYASS1UaNKA"
   },
   "outputs": [],
   "source": [
    "def display_new_prediction(input_image, pred_mask, fig_size=5, show=True):\n",
    "\n",
    "  titles = ['Input Image', 'Predicted Mask']\n",
    "\n",
    "  fig = plt.figure(figsize=(2 * fig_size, fig_size))\n",
    "  for ii, (img, title) in enumerate(zip([input_image, pred_mask], titles)):\n",
    "    plt.subplot(1, 2, ii+1)\n",
    "    plt.title(title)\n",
    "    plt.imshow(tf.keras.preprocessing.image.array_to_img(img))\n",
    "    plt.axis('off')\n",
    "  plt.savefig(\n",
    "      displays_path.joinpath('prediction_{}.png'.format(datetime.now().strftime(\"%Y-%m-%d-%H:%M:%S\"))).as_posix(),\n",
    "      dpi=300\n",
    "      )\n",
    "  if not show:\n",
    "    plt.close();\n",
    "\n",
    "\n",
    "def load_image_to_tensor(filepath, format=\"jpg\"):\n",
    "  image_raw = tf.io.read_file(filepath)\n",
    "  if format == \"png\":\n",
    "    image_decode = tf.image.decode_png(image_raw, channels=3)\n",
    "  elif format ==\"jpg\":\n",
    "    image_decode = tf.image.decode_jpeg(image_raw, channels=3)\n",
    "  else:\n",
    "    print(\"invalid input format\")\n",
    "    return\n",
    "  image_float = tf.image.convert_image_dtype(image_decode, tf.float32)\n",
    "  image_resize = tf.image.resize(image_float, INPUT_SIZE, method='nearest')\n",
    "  image_4d = tf.expand_dims(image_resize, 0)\n",
    "  return image_4d\n",
    "\n",
    "\n",
    "def predict_from_file(model, filepath, format=\"jpg\"):\n",
    "  image_resize = load_image_to_tensor(filepath, format=\"jpg\")\n",
    "  print(\"Input image size:\", image_resize.shape)\n",
    "  pred_mask = model.predict(image_resize)\n",
    "  # pred_mask_reshape = tf.squeeze(pred_mask)\n",
    "  # print(\"Pred mask shape:\" pred_mask_reshape.shape)\n",
    "  display_new_prediction(tf.squeeze(image_resize), create_mask(pred_mask))\n",
    "  return pred_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ikrzoG24qwf5"
   },
   "outputs": [],
   "source": [
    "test_data_dir = code_root.joinpath('Semantic-Segmentation-Suite/data/real-world-examples')\n",
    "test_filename = \"example101.jpg\"\n",
    "test_filepath = test_data_dir.joinpath(test_filename).as_posix()\n",
    "print(\"Predicting mask for:\", test_filepath)\n",
    "# load_image_to_tensor(test_filepath, format=\"jpg\")\n",
    "predict_from_file(model, test_filepath, format=\"jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R24tahEqmSCk"
   },
   "source": [
    "## Next steps\n",
    "Now that you have an understanding of what image segmentation is and how it works, you can try this tutorial out with different intermediate layer outputs, or even different pretrained model. You may also challenge yourself by trying out the [Carvana](https://www.kaggle.com/c/carvana-image-masking-challenge/overview) image masking challenge hosted on Kaggle.\n",
    "\n",
    "You may also want to see the [Tensorflow Object Detection API](https://github.com/tensorflow/models/tree/master/research/object_detection) for another model you can retrain on your own data."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "private_outputs": true,
   "provenance": [
    {
     "file_id": "1oPnyqiIQyfggNlE5IcOUQQzqnlOK7DpV",
     "timestamp": 1615862726567
    },
    {
     "file_id": "1GHG4WG9V3IU4V-DyITpwhxF4AfOjWe9J",
     "timestamp": 1615147333290
    },
    {
     "file_id": "https://github.com/tensorflow/docs/blob/master/site/en/tutorials/images/segmentation.ipynb",
     "timestamp": 1615144093775
    }
   ]
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "fairplay",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
